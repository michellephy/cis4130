{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chicago \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, BooleanType, DateType, IntegerType, FloatType\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"downloads/chicago_data.csv\")\n",
    "\n",
    "df1 = df.drop('Case Number', 'Block', 'IUCR', 'Ward', 'Location Description', 'Arrest', 'Domestic', 'Beat', 'District', 'FBI Code', 'Community Area', 'Updated On', 'Description', 'Location', ' Updated On', 'Latitude', 'Longitude')\n",
    "\n",
    "\n",
    "df2 = df1.withColumn(\"Date\", col('Date').cast(DateType())).withColumn('ID', col('ID').cast(IntegerType())).withColumn('X Coordinate', col('X Coordinate').cast(FloatType())).withColumn('Y Coordinate', col('Y Coordinate').cast(FloatType())).withColumn('Year', col('Year').cast(IntegerType()))\n",
    "\n",
    "df3 = df2.filter(df.Year == 2018)\n",
    "\n",
    "df4 = df3.filter(df3.Year.isNotNull() & df3.Date.isNotNull() & df3.ID.isNotNull())\n",
    "\n",
    "chicago_cleaned = df4.filter(col('Primary Type').isNotNull()).filter(col('X Coordinate').isNotNull()).filter(col(\"Y Coordinate\").isNotNull())\n",
    "\n",
    "chicagocomplaint = chicago_cleaned.groupBy(\"Date\").count()\n",
    "\n",
    "## la \n",
    "\n",
    "dfa = spark.read.option(\"header\", \"true\").csv(\"downloads/la_data.csv\")\n",
    "\n",
    "df1a = dfa.drop('Report Type', 'Time', 'Area ID', 'Area Name', 'Reporting District', 'Age', 'Sex Code', 'Descent Code', 'Charge Group Code', 'Charge Group Description', 'Arrest Type Code', 'Charge', 'Disposition Description', 'Address', 'Cross Street', 'Location', 'Booking Date', 'Booking Time', 'Booking Location', 'Booking Location Code')\n",
    "\n",
    "\n",
    "df2a = df1a.withColumnRenamed(\"Arrest Date\",\"Date\").withColumnRenamed(\"Report ID\", \"ID\").withColumnRenamed(\"Charge Description\", \"Primary Type\").withColumnRenamed(\"LAT\", \"Latitude\").withColumnRenamed(\"LON\", \"Longitude\")\n",
    "\n",
    "\n",
    "df3a = df2a.withColumn(\"Date\", col('Date').cast(DateType())).withColumn('ID', col('ID').cast(IntegerType())).withColumn('Latitude', col('Latitude').cast(FloatType())).withColumn('Longitude', col('Longitude').cast(FloatType()))\n",
    "\n",
    "df4a = df3a.filter(df3a.Date > '2017-12-31')\n",
    "df4b = df4a.filter('2019-01-01' > df4a.Date)\n",
    "\n",
    "\n",
    "df5a = df4b.filter(col('Primary Type').isNotNull())\n",
    "\n",
    "la_cleaned = df5a.filter(df5a.ID.isNotNull() & df5a.Date.isNotNull() & df5a.Latitude.isNotNull() & df5a.Longitude.isNotNull())\n",
    "\n",
    "lacomplaint = la_cleaned.groupBy(\"Date\").count()\n",
    "\n",
    "## ny\n",
    "\n",
    "dfn = spark.read.option(\"header\", \"true\").csv(\"downloads/ny_data.csv\")\n",
    "\n",
    "df1n = dfn.drop('PD_DESC', 'PD_CD', 'KY_CD', 'LAW_CODE', 'LAW_CAT_CD', 'ARREST_BORO', 'ARREST_PRECINCT', 'JURISDICTION_CODE', 'AGE_GROUP', 'PERP_SEX', 'PERP_RACE', 'X_COORD_CD', 'Y_COORD_CD', 'Lon_Lat')\n",
    "\n",
    "df2n = df1n.withColumnRenamed(\"ARREST_KEY\", \"ID\").withColumnRenamed(\"ARREST_DATE\", \"Date\").withColumnRenamed(\"OFNS_DESC\", \"Primary Type\")\n",
    "\n",
    "df3n = df2n.withColumn(\"Date\", col('Date').cast(DateType())).withColumn('ID', col('ID').cast(IntegerType())).withColumn('Latitude', col('Latitude').cast(FloatType())).withColumn('Longitude', col('Longitude').cast(FloatType()))\n",
    "\n",
    "df4n = df3n.filter(df3n.Date > '2017-12-31')\n",
    "df4m = df4n.filter('2019-01-01' > df4n.Date)\n",
    "\n",
    "\n",
    "df5n = df4m.filter(col('Primary Type').isNotNull())\n",
    "\n",
    "ny_cleaned = df5n.filter(df5n.ID.isNotNull() & df5n.Date.isNotNull() & df5n.Latitude.isNotNull() & df5n.Longitude.isNotNull()) \n",
    "\n",
    "nycomplaint = ny_cleaned.groupBy(\"Date\").count()\n",
    "\n",
    "# chicago\n",
    "dfcw = spark.read.option(\"header\", \"true\").csv(\"downloads/chicago_weather.csv\")\n",
    "\n",
    "dfcw1 = dfcw.withColumnRenamed(\"time\", \"Date\")\n",
    "\n",
    "dfcw2 = dfcw1.drop('temperature_2m_max (°F)', 'temperature_2m_min (°F)', 'rain_sum (inch)', 'precipitation_sum (inch)', 'snowfall_sum (cm)')\n",
    "\n",
    "\n",
    "dfcw3 = dfcw2.withColumn(\"Date\", col('Date').cast(DateType())).withColumn('apparent_temperature_min (°F)', col('apparent_temperature_min (°F)').cast(FloatType())).withColumn('apparent_temperature_max (°F)', col('apparent_temperature_max (°F)').cast(FloatType()))\n",
    "\n",
    "\n",
    "# new york\n",
    "dfnw = spark.read.option(\"header\", \"true\").csv(\"downloads/ny_weather.csv\")\n",
    "\n",
    "dfnw1 = dfnw.withColumnRenamed(\"time\", \"Date\")\n",
    "\n",
    "dfnw2 = dfnw1.drop('temperature_2m_max (°F)', 'temperature_2m_min (°F)', 'rain_sum (inch)', 'precipitation_sum (inch)', 'snowfall_sum (cm)')\n",
    "\n",
    "dfnw3 = dfnw2.withColumn(\"Date\", col('Date').cast(DateType())).withColumn('apparent_temperature_min (°F)', col('apparent_temperature_min (°F)').cast(FloatType())).withColumn('apparent_temperature_max (°F)', col('apparent_temperature_max (°F)').cast(FloatType()))\n",
    "\n",
    "\n",
    "# los angeles \n",
    "dflw = spark.read.option(\"header\", \"true\").csv(\"downloads/la_weather.csv\")\n",
    "\n",
    "dflw1 = dflw.withColumnRenamed(\"time\", \"Date\")\n",
    "\n",
    "dflw2 = dflw1.drop('temperature_2m_max (°F)', 'temperature_2m_min (°F)', 'rain_sum (inch)', 'precipitation_sum (inch)', 'snowfall_sum (cm)')\n",
    "\n",
    "\n",
    "dflw3 = dflw2.withColumn(\"Date\", col('Date').cast(DateType())).withColumn('apparent_temperature_min (°F)', col('apparent_temperature_min (°F)').cast(FloatType())).withColumn('apparent_temperature_max (°F)', col('apparent_temperature_max (°F)').cast(FloatType()))\n",
    "\n",
    "\n",
    "\n",
    "# average temps\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "chicagoweather = dfcw3.withColumn(\"Average Apparent Temperature\", (col('apparent_temperature_max (°F)') + col('apparent_temperature_min (°F)'))/2)\n",
    "\n",
    "chicago_weather = chicagoweather.drop('apparent_temperature_min (°F)', 'apparent_temperature_max (°F)')\n",
    "\n",
    "nyweather = dfnw3.withColumn(\"Average Apparent Temperature\", (col('apparent_temperature_max (°F)') + col('apparent_temperature_min (°F)'))/2)\n",
    "\n",
    "ny_weather = nyweather.drop('apparent_temperature_min (°F)', 'apparent_temperature_max (°F)')\n",
    "\n",
    "laweather = dflw3.withColumn(\"Average Apparent Temperature\", (col('apparent_temperature_max (°F)') + col('apparent_temperature_min (°F)'))/2)\n",
    "\n",
    "la_weather = laweather.drop('apparent_temperature_min (°F)', 'apparent_temperature_max (°F)')\n",
    "\n",
    "# find monthly average temperature\n",
    "\n",
    "\n",
    "ny_avg_temp_per_month = avg_ny.groupBy(\"DateMonth\").avg(\"Average Apparent Temperature\")\n",
    "\n",
    "c_avg_temp_per_month = avg_chicago.groupBy(\"DateMonth\").avg(\"Average Apparent Temperature\")\n",
    "\n",
    "la_avg_temp_per_month = avg_la.groupBy(\"DateMonth\").avg(\"Average Apparent Temperature\")\n",
    "\n",
    "\n",
    "\n",
    "## FINAL DFS FOR MODELING \n",
    "\n",
    "newyork = ny_avgs.alias(\"a\").join(avg_ny.alias(\"b\"), col('a.DateMonth') == col('b.DateMonth')).drop(ny_avgs.DateMonth)\n",
    "newyork = newyork.drop(newyork.DateMonth)\n",
    "\n",
    "ny_df = newyork.withColumnRenamed(\"avg(count)\", 'AverageCrimesPerMonth').withColumnRenamed(\"avg(Average Apparent Temperature)\", \"AverageTemperaturePerMonth\").withColumnRenamed(\"count\", 'NumberOfCrimesPerDay').withColumnRenamed(\"Average Apparent Temperature\", 'DailyTemperature')\n",
    "\n",
    "ny_df = ny_df.select('Date', 'DailyTemperature', 'NumberOfCrimesPerDay', 'AverageCrimesPerMonth', 'AverageTemperaturePerMonth', \"Location\")\n",
    "\n",
    "chicago = c_avgs.alias(\"a\").join(avg_chicago.alias(\"b\"), col('a.DateMonth') == col('b.DateMonth')).drop(c_avgs.DateMonth)\n",
    "\n",
    "chicago = chicago.drop(chicago.DateMonth)\n",
    "\n",
    "chicago_df = chicago.withColumnRenamed(\"avg(count)\", 'AverageCrimesPerMonth').withColumnRenamed(\"avg(Average Apparent Temperature)\", \"AverageTemperaturePerMonth\").withColumnRenamed(\"count\", 'NumberOfCrimesPerDay').withColumnRenamed(\"Average Apparent Temperature\", 'DailyTemperature')\n",
    "\n",
    "chicago_df = chicago_df.select('Date', 'DailyTemperature', 'NumberOfCrimesPerDay', 'AverageCrimesPerMonth', 'AverageTemperaturePerMonth', \"Location\")\n",
    "\n",
    "la = la_avgs.alias(\"a\").join(avg_la.alias(\"b\"), col('a.DateMonth') == col('b.DateMonth')).drop(la_avgs.DateMonth)\n",
    "la = la.drop(la.DateMonth)\n",
    "\n",
    "la_df = la.withColumnRenamed(\"avg(count)\", 'AverageCrimesPerMonth').withColumnRenamed(\"avg(Average Apparent Temperature)\", \"AverageTemperaturePerMonth\").withColumnRenamed(\"count\", 'NumberOfCrimesPerDay').withColumnRenamed(\"Average Apparent Temperature\", 'DailyTemperature')\n",
    "\n",
    "la_df = la_df.select('Date', 'DailyTemperature', 'NumberOfCrimesPerDay', 'AverageCrimesPerMonth', 'AverageTemperaturePerMonth', \"Location\")\n",
    "\n",
    "## concatenated FINAL DATA \n",
    "\n",
    "combine1 = ny_df.unionByName(chicago_df)\n",
    "\n",
    "final = combine1.unionByName(la_df)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
